{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f989acf-5dc2-4c92-8740-4d3df06ccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751689ac-0077-4208-a18f-b4278b98d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_text(x: Optional[str]) -> str:\n",
    "    return x if isinstance(x, str) else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8488a2bc-5b7e-4349-a694-2e40a8173fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a: str, b: str) -> float:\n",
    "    sa = set(t.lower() for t in a.split())\n",
    "    sb = set(t.lower() for t in b.split())\n",
    "    if not sa and not sb:\n",
    "        return 1.0\n",
    "    if not sa or not sb:\n",
    "        return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb7a552-36e8-4684-a635-26eec374c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_l(hyp: str, ref: str) -> float:\n",
    "    try:\n",
    "        from rouge_score import rouge_scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        score = scorer.score(ref, hyp)[\"rougeL\"].fmeasure\n",
    "        return float(score)\n",
    "    except Exception:\n",
    "        # fallback: use jaccard if rouge unavailable\n",
    "        return jaccard(hyp, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f172fd-f300-44d6-a749-1115d2dc5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(x: float, lo: float, hi: float) -> float:\n",
    "    if hi <= lo:\n",
    "        return 0.0\n",
    "    return max(0.0, min(1.0, (x - lo) / (hi - lo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84139876-d05d-45f8-843c-9dab8322c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_reference(src: str, tgt: str, ref_flag: str, hyp: str, sim_fn) -> Tuple[str, str]:\n",
    "    \"\"\"Return (chosen_reference, reference_type_used) based on ref flag.\n",
    "    For 'either', we compute similarity to both and choose the more supportive one.\n",
    "    \"\"\"\n",
    "    src = safe_text(src)\n",
    "    tgt = safe_text(tgt)\n",
    "    if ref_flag == \"src\":\n",
    "        return src, \"src\"\n",
    "    if ref_flag == \"tgt\":\n",
    "        return tgt, \"tgt\"\n",
    "    # either: compare both; pick the one closer to hyp\n",
    "    s_src = sim_fn(hyp, src) if src else -1\n",
    "    s_tgt = sim_fn(hyp, tgt) if tgt else -1\n",
    "    if s_src >= s_tgt:\n",
    "        return src, \"src\"\n",
    "    else:\n",
    "        return tgt, \"tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b02c61e-f54e-4e7c-a5d4-0391557beac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Providers:\n",
    "    def __init__(self, device: Optional[str] = None):\n",
    "        self.device = device\n",
    "        self._sbert = None\n",
    "        self._nli_tok = None\n",
    "        self._nli_model = None\n",
    "        self._ppl_tok = None\n",
    "        self._ppl_model = None\n",
    "        self._lt = None\n",
    "        self._nlp = None\n",
    "\n",
    "\n",
    "    def sbert(self):\n",
    "        if self._sbert is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        return self._sbert\n",
    "    \n",
    "    \n",
    "    def nli(self):\n",
    "        if self._nli_model is None:\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "            self._nli_tok = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "            self._nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "            if self.device:\n",
    "                self._nli_model.to(self.device)\n",
    "        return self._nli_tok, self._nli_model\n",
    "\n",
    "\n",
    "    def ppl(self):\n",
    "        if self._ppl_model is None:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            self._ppl_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            self._ppl_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "            if self.device:\n",
    "                self._ppl_model.to(self.device)\n",
    "        return self._ppl_tok, self._ppl_model\n",
    "\n",
    "\n",
    "    def language_tool(self):\n",
    "        if self._lt is None:\n",
    "            import language_tool_python as ltp\n",
    "            try:\n",
    "                self._lt = ltp.LanguageTool(\"en-US\")\n",
    "            except Exception: \n",
    "                self._lt = None\n",
    "        return self._lt\n",
    "\n",
    "\n",
    "    def spacy(self):\n",
    "        if self._nlp is None:\n",
    "            import spacy\n",
    "            try:\n",
    "                self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except Exception: \n",
    "                try:\n",
    "                    import subprocess, sys\n",
    "                    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=False)\n",
    "                    self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "                except Exception:\n",
    "                    self._nlp = None\n",
    "        return self._nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008c8406-10c2-4cd2-978e-07baca3a0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityScorer:\n",
    "    def __init__(self, providers: Providers):\n",
    "        self.prov = providers\n",
    "    \n",
    "    \n",
    "    def sim(self, a: str, b: str) -> float:\n",
    "        a, b = safe_text(a), safe_text(b)\n",
    "        if not a or not b:\n",
    "            return 0.0\n",
    "        try:\n",
    "            model = self.prov.sbert()\n",
    "            emb = model.encode([a, b], convert_to_tensor=True)\n",
    "            # cosine similarity\n",
    "            import torch\n",
    "            sim = torch.nn.functional.cosine_similarity(emb[0:1], emb[1:2]).item()\n",
    "            return float(sim)\n",
    "        except Exception :\n",
    "            return jaccard(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f05a73-e8c5-4997-a800-afbf23f09f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIScorer:\n",
    "    def __init__(self, providers: Providers):\n",
    "        self.prov = providers\n",
    "    \n",
    "    \n",
    "    def entailment_contradiction(self, premise: str, hypothesis: str) -> Tuple[float, float, float]:\n",
    "        \"\"\"Return probabilities: (entailment, neutral, contradiction)\"\"\"\n",
    "        premise, hypothesis = safe_text(premise), safe_text(hypothesis)\n",
    "        if not premise or not hypothesis:\n",
    "            return 0.0, 1.0, 0.0\n",
    "        try:\n",
    "            tok, model = self.prov.nli()\n",
    "            import torch\n",
    "            batch = tok([premise], [hypothesis], return_tensors=\"pt\", truncation=True)\n",
    "            if self.prov.device:\n",
    "                batch = {k: v.to(self.prov.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                logits = model(**batch).logits[0]\n",
    "                probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            # label order for roberta-large-mnli is: contradiction, neutral, entailment\n",
    "            p_contra, p_neutral, p_entail = float(probs[0]), float(probs[1]), float(probs[2])\n",
    "            return p_entail, p_neutral, p_contra\n",
    "        except Exception:\n",
    "            # Heuristic fallback using lexical cues\n",
    "            jac = jaccard(premise, hypothesis)\n",
    "            # crude mapping\n",
    "            p_entail = jac\n",
    "            p_contra = max(0.0, 1.0 - jac)\n",
    "            p_neutral = 1.0 - max(p_entail, p_contra)\n",
    "            return p_entail, p_neutral, p_contra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37646a8-8b83-4469-86e4-0173090e4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinguisticScorer:\n",
    "    def __init__(self, providers: Providers, ppl_hi: float = 80.0):\n",
    "        self.prov = providers\n",
    "        self.ppl_hi = ppl_hi\n",
    "    \n",
    "    \n",
    "    def perplexity(self, text: str) -> Optional[float]:\n",
    "        text = safe_text(text)\n",
    "        if not text:\n",
    "            return None\n",
    "        try:\n",
    "            tok, model = self.prov.ppl()\n",
    "            import torch\n",
    "            enc = tok(text, return_tensors=\"pt\")\n",
    "            if self.prov.device:\n",
    "                enc = {k: v.to(self.prov.device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**enc, labels=enc[\"input_ids\"]) # LM loss\n",
    "                loss = outputs.loss.item()\n",
    "            ppl = math.exp(loss)\n",
    "            return float(ppl)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def grammar_errors(self, text: str) -> Optional[int]:\n",
    "        try:\n",
    "            lt = self.prov.language_tool()\n",
    "            if lt is None:\n",
    "                return None\n",
    "            matches = lt.check(text)\n",
    "            return int(len(matches))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def score(self, hyp: str, ref: str) -> float:\n",
    "        \"\"\"Return linguistic hallucination score (0..1). Higher = worse language / drift.\"\"\"\n",
    "        # Low overlap and high perplexity / many grammar errors imply higher linguistic hallucination risk\n",
    "        rl = rouge_l(hyp, ref) # 0..1, higher is better\n",
    "        jac = jaccard(hyp, ref)\n",
    "        ppl = self.perplexity(hyp)\n",
    "        errs = self.grammar_errors(hyp)\n",
    "        \n",
    "        \n",
    "        # Normalize to 0..1 risk components\n",
    "        drift = 1.0 - max(rl, jac) # low overlap => higher drift\n",
    "        ppl_risk = 0.0 if ppl is None else minmax_scale(ppl, lo=20.0, hi=self.ppl_hi)\n",
    "        err_risk = 0.0 if errs is None else min(1.0, errs / 10.0) # cap at 10 errors\n",
    "        \n",
    "        \n",
    "        # Weighted combination\n",
    "        risk = 0.45 * drift + 0.35 * ppl_risk + 0.20 * err_risk\n",
    "        return float(max(0.0, min(1.0, risk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b8c7df-b178-4db7-a161-044f97aa6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactualScorer:\n",
    "    def __init__(self, providers: Providers, wiki_check: bool = False):\n",
    "        self.prov = providers\n",
    "        self.wiki_check = wiki_check\n",
    "\n",
    "\n",
    "    def entity_support_ratio(self, hyp: str, ref: str) -> Optional[float]:\n",
    "        \"\"\"Return ratio of HYP named entities that appear (string match) in REF.\n",
    "        If no entities in hyp, return None (no factual claim detected).\n",
    "        \"\"\"\n",
    "        nlp = self.prov.spacy()\n",
    "        if nlp is None:\n",
    "            return None\n",
    "        hyp_doc = nlp(hyp)\n",
    "        ref_low = ref.lower()\n",
    "        ents = [e.text for e in hyp_doc.ents if e.label_ not in {\"CARDINAL\"}]\n",
    "        if not ents:\n",
    "            return None\n",
    "        supported = 0\n",
    "        for e in ents:\n",
    "            if e.strip() and e.strip().lower() in ref_low:\n",
    "                supported += 1\n",
    "        return supported / max(1, len(ents))\n",
    "\n",
    "\n",
    "    def score(self, hyp: str, ref: str, p_entail: float, p_contra: float) -> float:\n",
    "        \"\"\"Factual hallucination score (0..1). Higher = more likely factually wrong.\n",
    "        Combines: low entity support, low entailment, high contradiction.\n",
    "        \"\"\"\n",
    "        esr = self.entity_support_ratio(hyp, ref)\n",
    "        # If we cannot compute ESR or there are no entities, rely on NLI only\n",
    "        lack_support = 1.0 - esr if esr is not None else 0.5\n",
    "        nli_risk = max(1.0 - p_entail, p_contra)\n",
    "        risk = 0.55 * lack_support + 0.45 * nli_risk\n",
    "        return float(max(0.0, min(1.0, risk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab59210-78bd-45de-a085-5a52bdc0a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Weights:\n",
    "    w_ling: float = 0.20\n",
    "    w_log: float = 0.35\n",
    "    w_fact: float = 0.30\n",
    "    w_ctx: float = 0.15\n",
    "    \n",
    "    \n",
    "    def normalize(self):\n",
    "        s = self.w_ling + self.w_log + self.w_fact + self.w_ctx\n",
    "        if s <= 0:\n",
    "            self.w_ling = self.w_log = self.w_fact = self.w_ctx = 0.25\n",
    "            return\n",
    "        self.w_ling /= s\n",
    "        self.w_log /= s\n",
    "        self.w_fact /= s\n",
    "        self.w_ctx /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1a7847-ae26-4767-8341-9781581e9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_row(row: Dict, prov: Providers, weights: Weights, sim_threshold: float, ppl_hi: float) -> Dict:\n",
    "    sim_scorer = SimilarityScorer(prov)\n",
    "    nli_scorer = NLIScorer(prov)\n",
    "    ling_scorer = LinguisticScorer(prov, ppl_hi=ppl_hi)\n",
    "    fact_scorer = FactualScorer(prov)\n",
    "    ctx_scorer = ContextualScorer(sim_threshold=sim_threshold)\n",
    "\n",
    "\n",
    "    hyp = safe_text(row.get(\"hyp\"))\n",
    "    src = safe_text(row.get(\"src\"))\n",
    "    tgt = safe_text(row.get(\"tgt\"))\n",
    "    ref_flag = safe_text(row.get(\"ref\")) or \"either\"\n",
    "\n",
    " \n",
    "    sim_fn = lambda a, b: sim_scorer.sim(a, b)\n",
    "    ref_text, ref_used = choose_reference(src, tgt, ref_flag, hyp, sim_fn)\n",
    "\n",
    "\n",
    "    sim = sim_fn(hyp, ref_text)\n",
    "    p_entail, p_neutral, p_contra = nli_scorer.entailment_contradiction(ref_text, hyp)\n",
    "\n",
    "\n",
    "    ling = ling_scorer.score(hyp, ref_text)\n",
    "    logi = p_contra # logical hallucination proxied by contradiction prob\n",
    "    fact = fact_scorer.score(hyp, ref_text, p_entail, p_contra)\n",
    "    ctx = ctx_scorer.score(sim, p_entail, p_contra)\n",
    "\n",
    "\n",
    "    weights.normalize()\n",
    "    overall = (\n",
    "        weights.w_ling * ling +\n",
    "        weights.w_log * logi +\n",
    "        weights.w_fact * fact +\n",
    "        weights.w_ctx * ctx\n",
    "    )\n",
    "    label = \"Hallucination\" if overall >= 0.5 else \"Not Hallucination\"\n",
    "\n",
    "\n",
    "    out = {\n",
    "        \"task\": safe_text(row.get(\"task\")),\n",
    "        \"ref_flag\": ref_flag,\n",
    "        \"ref_used\": ref_used,\n",
    "        \"sim\": round(float(sim), 4),\n",
    "        \"p_entail\": round(float(p_entail), 4),\n",
    "        \"p_neutral\": round(float(p_neutral), 4),\n",
    "        \"p_contra\": round(float(p_contra), 4),\n",
    "        \"linguistic_p\": round(float(ling), 4),\n",
    "        \"logical_p\": round(float(logi), 4),\n",
    "        \"factual_p\": round(float(fact), 4),\n",
    "        \"contextual_p\": round(float(ctx), 4),\n",
    "        \"hallucination_p\": round(float(overall), 4),\n",
    "        \"pred_label\": label,\n",
    "    }\n",
    "    # pass through ground truth if present (for evaluation)\n",
    "    if \"label\" in row:\n",
    "        out[\"gold_label\"] = row.get(\"label\")\n",
    "    if \"p(Hallucination)\" in row:\n",
    "        out[\"gold_p\"] = row.get(\"p(Hallucination)\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee6d5f6-b4bf-4694-b44a-f2ece9d4146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    \n",
    "    \n",
    "    if not {\"gold_label\", \"pred_label\"}.issubset(df.columns):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "    # Binary accuracy / F1\n",
    "    y_true = (df[\"gold_label\"] == \"Hallucination\").astype(int).values\n",
    "    y_pred = (df[\"pred_label\"] == \"Hallucination\").astype(int).values\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    f1 = float(f1_score(y_true, y_pred))\n",
    "    \n",
    "    \n",
    "    # Correlation of probabilities\n",
    "    pear, spear = None, None\n",
    "    if \"gold_p\" in df.columns:\n",
    "        try:\n",
    "            pear = float(pearsonr(df[\"gold_p\"].astype(float), df[\"hallucination_p\"].astype(float))[0])\n",
    "            spear = float(spearmanr(df[\"gold_p\"].astype(float), df[\"hallucination_p\"].astype(float))[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"pearson\": pear, \"spearman\": spear}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff7031f6-2058-47b1-a098-1f69fe28a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--input\", required=True, help=\"Path to SHROOM JSON list file\")\n",
    "    ap.add_argument(\"--output\", required=True, help=\"Where to write scored CSV\")\n",
    "    ap.add_argument(\"--device\", default=None, help=\"e.g., cuda:0 or cpu\")\n",
    "    ap.add_argument(\"--evaluate\", action=\"store_true\", help=\"If labels exist, compute metrics\")\n",
    "\n",
    "\n",
    "# knobs\n",
    "    ap.add_argument(\"--w_ling\", type=float, default=0.20)\n",
    "    ap.add_argument(\"--w_log\", type=float, default=0.35)\n",
    "    ap.add_argument(\"--w_fact\", type=float, default=0.30)\n",
    "    ap.add_argument(\"--w_ctx\", type=float, default=0.15)\n",
    "    ap.add_argument(\"--sim_threshold\", type=float, default=0.70)\n",
    "    ap.add_argument(\"--ppl_hi\", type=float, default=80.0)\n",
    "\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "\n",
    "    with open(args.input, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "    df_in = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    prov = Providers(device=args.device)\n",
    "    weights = Weights(args.w_ling, args.w_log, args.w_fact, args.w_ctx)\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for row in tqdm(df_in.to_dict(orient=\"records\"), total=len(df_in)):\n",
    "        try:\n",
    "            out = score_row(row, prov, weights, sim_threshold=args.sim_threshold, ppl_hi=args.ppl_hi)\n",
    "        except Exception as e:\n",
    "            out = {\n",
    "                \"task\": row.get(\"task\"),\n",
    "                \"ref_flag\": row.get(\"ref\"),\n",
    "                \"ref_used\": None,\n",
    "                \"sim\": np.nan,\n",
    "                \"p_entail\": np.nan,\n",
    "                \"p_neutral\": np.nan,\n",
    "                \"p_contra\": np.nan,\n",
    "                \"linguistic_p\": np.nan,\n",
    "                \"logical_p\": np.nan,\n",
    "                \"factual_p\": np.nan,\n",
    "                \"contextual_p\": np.nan,\n",
    "                \"hallucination_p\": np.nan,\n",
    "                \"pred_label\": \"ERROR\",\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "            if \"label\" in row:\n",
    "                out[\"gold_label\"] = row.get(\"label\")\n",
    "            if \"p(Hallucination)\" in row:\n",
    "                out[\"gold_p\"] = row.get(\"p(Hallucination)\")\n",
    "        rows.append(out)\n",
    "\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    df_out.to_csv(args.output, index=False)\n",
    "\n",
    "\n",
    "    if args.evaluate and (\"gold_label\" in df_out.columns):\n",
    "        metrics = evaluate(df_out)\n",
    "        if metrics:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            for k, v in metrics.items():\n",
    "                if v is not None:\n",
    "                    print(f\" {k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(\"No evaluation performed (labels not present).\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63a9293d-7fff-428a-8a22-af16e97c05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nitish\n",
      "[nltk_data]     Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nitish Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is ready ✅\n"
     ]
    }
   ],
   "source": [
    "import nltk, spacy\n",
    "\n",
    "# NLTK resources (you already downloaded, but safe to run again)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"spaCy is ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44512630-6fbe-40cc-a043-b8315cf22f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programing\\HackLLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Nitish\n",
      "[nltk_data]     Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nitish Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Setup NLTK + spaCy ---\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- Core class ---\n",
    "class HallucinationDetector:\n",
    "    def __init__(self, sim_threshold=0.75, ppl_hi=60.0,\n",
    "                 w_ling=0.25, w_log=0.25, w_fact=0.25, w_ctx=0.25):\n",
    "        self.sim_threshold = sim_threshold\n",
    "        self.ppl_hi = ppl_hi\n",
    "        self.weights = {\n",
    "            \"linguistic\": w_ling,\n",
    "            \"logical\": w_log,\n",
    "            \"factual\": w_fact,\n",
    "            \"contextual\": w_ctx,\n",
    "        }\n",
    "        # Load embeddings model\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        # (Optional) for perplexity / fluency scoring\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        self.lm = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "    def linguistic_score(self, hyp, tgt):\n",
    "        # basic edit-distance style (can replace w/ BLEU, chrF, etc.)\n",
    "        hyp_tokens = nltk.word_tokenize(hyp)\n",
    "        tgt_tokens = nltk.word_tokenize(tgt)\n",
    "        overlap = len(set(hyp_tokens) & set(tgt_tokens))\n",
    "        return overlap / (len(set(tgt_tokens)) + 1e-5)\n",
    "\n",
    "    def logical_score(self, hyp, src):\n",
    "        # use embeddings cosine similarity\n",
    "        emb = self.embedder.encode([hyp, src])\n",
    "        sim = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "        return sim\n",
    "\n",
    "    def factual_score(self, hyp, ref):\n",
    "        # placeholder: treat similarity to reference as factual alignment\n",
    "        emb = self.embedder.encode([hyp, ref])\n",
    "        sim = cosine_similarity([emb[0]], [emb[1]])[0][0]\n",
    "        return sim\n",
    "\n",
    "    def contextual_score(self, hyp, src):\n",
    "        # cheap syntax complexity proxy\n",
    "        doc = nlp(hyp)\n",
    "        depth = max([token.dep_ for token in doc], default=\"\")  # just presence of deps\n",
    "        return 1.0 if depth else 0.5\n",
    "\n",
    "    def classify(self, row):\n",
    "        hyp, src, tgt, ref = row[\"hyp\"], row[\"src\"], row[\"tgt\"], row[\"ref\"]\n",
    "\n",
    "        ling = self.linguistic_score(hyp, tgt)\n",
    "        logi = self.logical_score(hyp, src)\n",
    "        fact = self.factual_score(hyp, ref if ref else src)\n",
    "        ctx  = self.contextual_score(hyp, src)\n",
    "\n",
    "        # weighted sum\n",
    "        score = (\n",
    "            ling * self.weights[\"linguistic\"] +\n",
    "            logi * self.weights[\"logical\"] +\n",
    "            fact * self.weights[\"factual\"] +\n",
    "            ctx  * self.weights[\"contextual\"]\n",
    "        )\n",
    "\n",
    "        label = \"Hallucination\" if score < self.sim_threshold else \"Not Hallucination\"\n",
    "\n",
    "        return pd.Series({\n",
    "            \"linguistic_score\": ling,\n",
    "            \"logical_score\": logi,\n",
    "            \"factual_score\": fact,\n",
    "            \"contextual_score\": ctx,\n",
    "            \"p(Hallucination)\": float(1 - score),\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "# --- Convenience wrapper ---\n",
    "def run_pipeline(input_path, output_path=None, evaluate=False):\n",
    "    df = pd.read_json(input_path)\n",
    "    detector = HallucinationDetector()\n",
    "\n",
    "    scores = df.apply(detector.classify, axis=1)\n",
    "    df_out = pd.concat([df, scores], axis=1)\n",
    "\n",
    "    if output_path:\n",
    "        df_out.to_csv(output_path, index=False)\n",
    "        print(f\"[+] Results saved to {output_path}\")\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8661aafa-0584-4414-a771-d834fde6251e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Nitish\n",
      "[nltk_data]     Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Nitish\n",
      "[nltk_data]     Jangra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2d9f94-e4f5-48bb-8ca9-e2458b147f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Results saved to ./data/train_scored.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>linguistic_score</th>\n",
       "      <th>logical_score</th>\n",
       "      <th>factual_score</th>\n",
       "      <th>contextual_score</th>\n",
       "      <th>p(Hallucination)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.874999</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>0.184666</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.477711</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.699999</td>\n",
       "      <td>-0.069661</td>\n",
       "      <td>0.152298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554341</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.120228</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656854</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.166666</td>\n",
       "      <td>0.109546</td>\n",
       "      <td>0.163692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640024</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.699999</td>\n",
       "      <td>-0.051613</td>\n",
       "      <td>0.046931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576171</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  \\\n",
       "0          Не волнуйся. Это только временно.  either   MT         \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT         \n",
       "2               Мне сложно работать с Томом.  either   MT         \n",
       "3                          Воду, пожалуйста.  either   MT         \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT         \n",
       "\n",
       "   linguistic_score  logical_score  factual_score  contextual_score  \\\n",
       "0          0.874999       0.029491       0.184666               1.0   \n",
       "1          0.699999      -0.069661       0.152298               1.0   \n",
       "2          0.428571      -0.120228       0.064242               1.0   \n",
       "3          0.166666       0.109546       0.163692               1.0   \n",
       "4          0.699999      -0.051613       0.046931               1.0   \n",
       "\n",
       "   p(Hallucination)          label  \n",
       "0          0.477711  Hallucination  \n",
       "1          0.554341  Hallucination  \n",
       "2          0.656854  Hallucination  \n",
       "3          0.640024  Hallucination  \n",
       "4          0.576171  Hallucination  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = run_pipeline(\"./data/train.model-agnostic.json\", \"./data/train_scored.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf587e7c-21ae-4012-8128-ff6925ebf666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>linguistic_score</th>\n",
       "      <th>logical_score</th>\n",
       "      <th>factual_score</th>\n",
       "      <th>contextual_score</th>\n",
       "      <th>p(Hallucination)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.874999</td>\n",
       "      <td>0.029491</td>\n",
       "      <td>0.184666</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.477711</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.699999</td>\n",
       "      <td>-0.069661</td>\n",
       "      <td>0.152298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554341</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.120228</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656854</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.166666</td>\n",
       "      <td>0.109546</td>\n",
       "      <td>0.163692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640024</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>0.699999</td>\n",
       "      <td>-0.051613</td>\n",
       "      <td>0.046931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576171</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>Yeah, I'm listening.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, I'm listening.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.462816</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Time?</td>\n",
       "      <td></td>\n",
       "      <td>The time?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838017</td>\n",
       "      <td>0.163360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499656</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>Plague?</td>\n",
       "      <td></td>\n",
       "      <td>A plague?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921636</td>\n",
       "      <td>0.092533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>Tango, Tango.</td>\n",
       "      <td></td>\n",
       "      <td>Tango.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952118</td>\n",
       "      <td>0.179537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.467086</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>It's either him or me.</td>\n",
       "      <td></td>\n",
       "      <td>Him or me.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803524</td>\n",
       "      <td>0.140650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.513956</td>\n",
       "      <td>Hallucination</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      hyp  \\\n",
       "0       Don't worry, it's only temporary.   \n",
       "1        Tom is never where he should be.   \n",
       "2      It's hard for me to work with Tom.   \n",
       "3                          Water, please.   \n",
       "4       I didn't expect Tom to betray me.   \n",
       "...                                   ...   \n",
       "29995                Yeah, I'm listening.   \n",
       "29996                               Time?   \n",
       "29997                             Plague?   \n",
       "29998                       Tango, Tango.   \n",
       "29999              It's either him or me.   \n",
       "\n",
       "                                            tgt  \\\n",
       "0             Don't worry. It's only temporary.   \n",
       "1       Tom is never where he's supposed to be.   \n",
       "2              I have trouble working with Tom.   \n",
       "3                          I'd like some water.   \n",
       "4      I didn't think that Tom would betray me.   \n",
       "...                                         ...   \n",
       "29995                                             \n",
       "29996                                             \n",
       "29997                                             \n",
       "29998                                             \n",
       "29999                                             \n",
       "\n",
       "                                             src     ref task model  \\\n",
       "0              Не волнуйся. Это только временно.  either   MT         \n",
       "1      Тома никогда нет там, где он должен быть.  either   MT         \n",
       "2                   Мне сложно работать с Томом.  either   MT         \n",
       "3                              Воду, пожалуйста.  either   MT         \n",
       "4             Я не ожидал, что Том предаст меня.  either   MT         \n",
       "...                                          ...     ...  ...   ...   \n",
       "29995                       Yeah, I'm listening.     src   PG         \n",
       "29996                                  The time?     src   PG         \n",
       "29997                                  A plague?     src   PG         \n",
       "29998                                     Tango.     src   PG         \n",
       "29999                                 Him or me.     src   PG         \n",
       "\n",
       "       linguistic_score  logical_score  factual_score  contextual_score  \\\n",
       "0              0.874999       0.029491       0.184666               1.0   \n",
       "1              0.699999      -0.069661       0.152298               1.0   \n",
       "2              0.428571      -0.120228       0.064242               1.0   \n",
       "3              0.166666       0.109546       0.163692               1.0   \n",
       "4              0.699999      -0.051613       0.046931               1.0   \n",
       "...                 ...            ...            ...               ...   \n",
       "29995          0.000000       1.000000       0.148736               1.0   \n",
       "29996          0.000000       0.838017       0.163360               1.0   \n",
       "29997          0.000000       0.921636       0.092533               1.0   \n",
       "29998          0.000000       0.952118       0.179537               1.0   \n",
       "29999          0.000000       0.803524       0.140650               1.0   \n",
       "\n",
       "       p(Hallucination)          label  \n",
       "0              0.477711  Hallucination  \n",
       "1              0.554341  Hallucination  \n",
       "2              0.656854  Hallucination  \n",
       "3              0.640024  Hallucination  \n",
       "4              0.576171  Hallucination  \n",
       "...                 ...            ...  \n",
       "29995          0.462816  Hallucination  \n",
       "29996          0.499656  Hallucination  \n",
       "29997          0.496458  Hallucination  \n",
       "29998          0.467086  Hallucination  \n",
       "29999          0.513956  Hallucination  \n",
       "\n",
       "[30000 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11bba8-fdce-4864-a1a4-6368d91af925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
